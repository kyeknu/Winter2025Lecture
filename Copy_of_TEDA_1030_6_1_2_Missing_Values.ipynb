{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kyeknu/Winter2025Lecture/blob/main/Copy_of_TEDA_1030_6_1_2_Missing_Values.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6.1.2 Missing Values\n",
        "\n",
        "Missing values, also called *null values* indicates the absence of data. Null values can occur because of data that was inputting incorrectly or incompletely and can affect the analysis by skewing data incorrectly and giving incorrect insights. Null values also cannot be processed by machine learning models, which means that they cannot be present in order for machine learning to work.\n",
        "\n",
        "Missing data can be treated with several different strategies, depending on the information available to the analyst and the importance of recovering the data.\n",
        "\n",
        "1. Deletion: Rows with null values in a certain column or columns with many null values can be removed. However, removing rows and columns will also lead to a loss of data in those rows or columns. For example, in an analysis of grades in a classroom of 30 students, if only two assignments were turned in and were recorded as 100% and 98%, it may be misleading to say that the average score is 99% (although it is technically true).\n",
        "2. Imputation: If other information can be used to fill in empty spaces, the information can simply be filled in, or imputed. This prevents a loss of data but is often difficult to do if the information is not contained somewhere else. For example, if an row (representing an order) is missing its final price but contains a unit cost and quantity, the total price could simply be computed and the blank space filled in.\n",
        "3. Prediction: Machine learning models can be used to make predictions and fill in empty spaces. This is not as accurate as imputation but can help preserve data integrity in some analyses. For example, in an analysis of house prices, a model could be created that helps estimate the price of a house based on the number of bedrooms and bathrooms it has. This would not affect the measure of center for the houses but is not as accurate as just knowing what the actual price is. We will not touch on prediction in this class.\n",
        "\n",
        "\n",
        "\n",
        "Let's learn to treat missing values by looking at a modified version of the Titanic data.\n",
        "\n",
        "## About the data\n",
        "This data set is a modified version of the classic Titanic data set. It contains many kinds of errors, which will be used to demonstrate cleaning concepts in this reading.\n",
        "\n",
        "âš  Warning: Remember to upload the Titanic Dirty data set to Google Colab before running the cells below."
      ],
      "metadata": {
        "id": "H1kvCrabo2HD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv('titanic_dirty.csv')\n",
        "df.head()"
      ],
      "metadata": {
        "id": "sXM3zbcr1e7c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Identifying missing values\n",
        "\n",
        "Right from the beginning, it's easy to see some null values in this data set in the `Age`, `Fare`, and `Cabin` columns. Null values are marked by `NaN`, meaning \"not a number\".\n",
        "\n",
        "We can check how many null values there are in each column by using the `.info()` method on the dataframe.\n"
      ],
      "metadata": {
        "id": "nhcElQij1aAX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qmiyTv17ozYY"
      },
      "outputs": [],
      "source": [
        "df.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This method gives us some important information. First, it says on the second line that there are 978 entries (rows) in total. Then, it lists each column and the number of non-null values that each one has. In the `Sex` column, for example, there are 819 non-null values, meaning that there are 159 null values in the column."
      ],
      "metadata": {
        "id": "T7Qis50N18WG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dropping null values\n",
        "\n",
        "We can drop null values using the `.dropna()` method. Remember that the resulting dataframe will not be saved unless the `inplace=True` parameter is passed in."
      ],
      "metadata": {
        "id": "SGEsO3C22T55"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.dropna() # Not saving the resulting dataframe on purpose"
      ],
      "metadata": {
        "id": "_2HO6vpB2RUN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Observe the data set above. Thanks to the `.dropna()` method, all of the rows that had even a single null value were dropped. So how many rows do we have left?\n",
        "\n",
        "Looking at the lower left corner of the dataframe above, we can see that after dropping all null values, we are left with only 144 rows. That's only 14% of the original data set!\n",
        "\n",
        "The results above exemplify an important dilemma. By eliminating the null values in the data set, we've also eliminated a lot of good data too. So, how many null values should be cut out?\n",
        "\n",
        "The answer is extremely subjective and dependent on the analysis. For an analysis that compares fares and ages, the `Cabin` column probably won't be very important. However, a lot of rows from above that actually had `Fare` and `Age` recorded correctly were dropped because they didn't list a `Cabin`. In that analysis, a better decision would probably be to drop the entire `Cabin` column first and then try to see how many null values were left.\n",
        "\n"
      ],
      "metadata": {
        "id": "rAP6Co8P2ZuW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dropping null values with a subset\n",
        "\n",
        "Just like dropping duplicates, null values can be dropped using a subset of columns to look for null values in. For example, for performing an analysis of `Fare` and `Age`, null values in the `Cabin` column probably don't matter."
      ],
      "metadata": {
        "id": "-VXTCPTn3xig"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.dropna(subset=['Age', 'Fare'])"
      ],
      "metadata": {
        "id": "hFB9fYVS2104"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "By only dropping rows that had a null value either in the Age or Fare column, 638 rows were conserved for analysis."
      ],
      "metadata": {
        "id": "J938R_zZ4CS3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dropping specific columns/rows\n",
        "\n",
        "In some cases, it may be necessary to remove a column from the dataframe. To do so, you can use the `.drop()` method and the `columns=` parameter, where columns is a list of column names to drop. Remember to use `inplace=True` to save the new dataframe."
      ],
      "metadata": {
        "id": "4lqdeU4I4WXO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Notice the `Cabin` column is gone\n",
        "df.drop(columns=['Cabin'])"
      ],
      "metadata": {
        "id": "YbNtvAIh4BOt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can also drop specific rows from the dataframe using `.drop()` and the `index=` parameter, where index is a list of row numbers to drop. Dropping specific rows is a very uncommon task to perform, but could be done as follows:"
      ],
      "metadata": {
        "id": "1jW2-HJw4qyE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Notice rows 1 and 3 are gone\n",
        "df.drop(index=[1, 3])"
      ],
      "metadata": {
        "id": "Vo7oMjnu4prQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Imputation\n",
        "\n",
        "Imputation is a subjective process in which empty values are computed using data that exists in the data set. For example, missing fares could be imputd using the median fare in the data set."
      ],
      "metadata": {
        "id": "ECGZtZcP5as6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the median fare\n",
        "df['Fare'].median()"
      ],
      "metadata": {
        "id": "-jx45Khy4-N3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "When the value to be filled in is found, the `.fillna()` method can be used to fill in the null values of the column."
      ],
      "metadata": {
        "id": "f6v4t__c5svi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Fill in null Fares with the median fare -- notice row 4, which was previously a null fare!\n",
        "df['Fare'] = df['Fare'].fillna(df['Fare'].median())\n",
        "df.head()"
      ],
      "metadata": {
        "id": "qXrcwgrm5QBq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You could also use a more complicated process to impute values. For example, notice that the `Sex` column contains 159 null values, but the `Name` column contains information regarding the passenger's `Sex` (Mr., Mrs., Rev., Miss). Because the information is contained in the data set, we can use it to impute the `Sex of each passenger."
      ],
      "metadata": {
        "id": "K5rvh14f6Ife"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract Mr., Rev., Mrs., or Miss using a regular expression and save to a new column\n",
        "df['Title'] = df['Name'].str.extract('(Mr.|Mrs.|Rev.|Miss.)')[0]\n",
        "df.head()"
      ],
      "metadata": {
        "id": "Vs4n0qx154FK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use a for-loop to go through each row of the dataframe\n",
        "# Remember that i represents the row number and row represents the row data\n",
        "\n",
        "for i, row in df.iterrows():\n",
        "\n",
        "    # If a title was found for the person and the title was 'Mr.'\n",
        "    if row['Title'] == 'Mr.':\n",
        "        # Change the `Sex` of that row to 'male'\n",
        "        df.loc[i, 'Sex'] = 'male'\n",
        "\n",
        "    # If a title was found for the person and the title was 'Mrs.'\n",
        "    if row['Title'] == 'Mrs.':\n",
        "        # Change the `Sex` of that row to 'female'\n",
        "        df.loc[i, 'Sex'] = 'female'\n",
        "\n",
        "    # If a title was found for the person and the title was 'Miss.'\n",
        "    if row['Title'] == 'Miss.':\n",
        "        # Change the `Sex` of that row to 'female'\n",
        "        df.loc[i, 'Sex'] = 'female'\n",
        "\n",
        "    # If a title was found for the person and the title was 'Mrs.'\n",
        "    if row['Title'] == 'Rev.':\n",
        "        # Change the `Sex` of that row to 'male'\n",
        "        df.loc[i, 'Sex'] = 'male'"
      ],
      "metadata": {
        "id": "JC4t9gqU67Wg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['Sex'].value_counts()"
      ],
      "metadata": {
        "id": "1knJlay14Wlj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A for loop was used to go through all of the data, check to see if a title was found, and then change the `Sex`. Let's see how many null values are in that column now:"
      ],
      "metadata": {
        "id": "EEfQ4Ucj8bIb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()"
      ],
      "metadata": {
        "id": "Qu9vtvUo7sRA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "After performing that more complex process, there are 948 non-null values in the `Sex` column. That means that there are only 30 null values remaining. In other words, we were able to impute a large number of values in `Sex` column, but not all of them. We could keep perfecting the code an regular expression until we got a better result, if we wanted to.\n",
        "\n",
        "This has been a short reading on fixing missing values. Missing values are common in data sets but are oftentimes fixable either by removing the values that are null or by imputing the values that are currently there."
      ],
      "metadata": {
        "id": "PupaDZqe8oY0"
      }
    }
  ]
}