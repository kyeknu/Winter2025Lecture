{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 6.1.1 Duplicates\n",
        "\n",
        "Duplicate values in data analytics refer to identical or nearly identical records or entries within a dataset. These repetitions can arise from various sources such as data entry errors, system glitches, or merging multiple data sources. Addressing duplicate values is crucial as they can distort analysis results, lead to inaccurate insights, and waste computational resources.\n",
        "\n",
        "Treating duplicate values can invovle several steps, but mainly involves identifying the duplicate entries and then deciding what to do with each one.\n",
        "\n",
        "Let's start by looking at a modified version of the *Titanic* data.\n",
        "\n",
        "## About the data\n",
        "This data set is a modified version of the classic *Titanic* data set. It contains many kinds of errors, which will be used to demonstrate cleaning concepts in this reading.\n",
        "\n",
        "âš  Warning: Remember to upload the Titanic Dirty data set to Google Colab before running the cells below."
      ],
      "metadata": {
        "id": "-mQxkjGvqI3L"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W9cbhJIod_U8"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv('titanic_dirty.csv')\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.shape"
      ],
      "metadata": {
        "id": "DAdnwd1j0FJb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Identifying duplicates\n",
        "\n",
        "Observe the data set above. In the first five rows of the data set, things look fairly normal (for the most part). Finding duplicates in any data set is difficult to do by just looking at the data, since there could be hundreds of thousands of rows.\n",
        "\n",
        "Pandas makes identifying duplicate rows easy using the `.duplicated()` method."
      ],
      "metadata": {
        "id": "YJMCMZSCrWZU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.duplicated()"
      ],
      "metadata": {
        "id": "a2uSHnBMrQGg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The `.duplicated()` method goes through each row of the dataframe and decides whether or not another row matches it **exactly**. If another row does exist that is exactly the same, the `.duplicated()` method marks that row as True.\n",
        "\n",
        "We can use `.duplicated()` to create a filter that shows all duplicated rows.\n",
        "\n",
        "Note: You can add the parameter `keep=False` to the `.duplicated()` method to show both of the rows at the same time, to verify that they are duplicates."
      ],
      "metadata": {
        "id": "J-2s0ECGtOLW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df[ df.duplicated() ].sort_values(by='Name')"
      ],
      "metadata": {
        "id": "zAnb0LfNsaAn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the code above, we can see that there were about 24 passengers whose rows were duplicated exactly. Great work!\n",
        "\n",
        "## Now what?\n",
        "Now that the duplicate entries have been identified, we have to decide what to do with them. Should we drop the duplicate passengers? What if there are two passengers who happen to have the same name and the same gender and paid the same fare and have the same age?\n",
        "\n",
        "The answer to these questions will determine your course of action. Most of the time, however, duplicate rows are simply dropped.\n",
        "\n",
        "## Dropping duplicates\n",
        "\n",
        "To drop duplicates, you can simply use the `.drop_duplicates()` method on a pandas dataframe. Note that running the function by itself will only show a preview of the data without duplicates, but won't save unless it is saved back to the variable `df` or is given the parameter `inplace=True`."
      ],
      "metadata": {
        "id": "RRni6-o9t7nF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.drop_duplicates(inplace=True)"
      ],
      "metadata": {
        "id": "qYrt7EVPtiCd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[ df.duplicated() ].sort_values(by='Name')"
      ],
      "metadata": {
        "id": "VQgWMlOozVf7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "That's it! The duplicates have been dropped from the data set.\n",
        "\n",
        "## Checking for duplicates by subset\n",
        "Dropping duplicates may have seemed easy, but remember that by default, the `.duplicated()` and `.drop_duplicates()` methods look for rows that are **exactly the same**. But what happens if the same passenger was entered into the data set twice, but with a different `Fare` each time? Because the rows aren't exactly the same, they won't be marked as duplicates.\n",
        "\n",
        "For this reason, we can use the `subset` argument to search for duplicates using one or a few columns instead of all the columns.\n",
        "\n",
        "Let's use some logic to determine which columns to look for duplicates in. We couldn't use the `Fare` column or `Age` columns because it is possible for two different passengers to have paid the same fare and have the same age. We *could* probably use the `Name` column, but technically, two passengers *could* have the exact same name. However, what are the chances that two passengers have the same `Name` and stayed in the same `Cabin`? Let's take a look at the passengers who meet that criteria using the `subset` parameter."
      ],
      "metadata": {
        "id": "GLVBtYDMwHPv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.loc[ df.duplicated(subset=['Name', 'Cabin'])]"
      ],
      "metadata": {
        "id": "rG3r8XiOuBeg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "By using the `subset` parameter and searching for duplicates only using the `Name` and `Fare` columns, we were able to identify an additional 63 duplicates in the data set. Now, we can use the `subset` argument to drop these passengers."
      ],
      "metadata": {
        "id": "MmkwV1J6xhio"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.drop_duplicates(subset=['Name', 'Cabin'], inplace=True)\n",
        "df.shape"
      ],
      "metadata": {
        "id": "1me10b14xe1V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Deciding which rows are duplicates is very subjective, so there isn't a great way to validate and check if all the duplicates were removed or not. It is likely impossible to always remove all duplicate entries from the data set, but by reducing the amount of duplicate entries the analysis will be more accurate than otherwise."
      ],
      "metadata": {
        "id": "LmfvAWVkx1JG"
      }
    }
  ]
}